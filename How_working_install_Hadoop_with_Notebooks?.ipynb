{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiE/2RSY42V1HY39+QIOmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xlmriosx/2022-TPI-G1-V2/blob/main/How_working_install_Hadoop_with_Notebooks%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üêòüìù Basic commands to work with Hadoop in Notebooks \n",
        "---\n",
        "## üîóRelated content \n",
        "### You can find post related in: \n",
        "üë®‚Äçüíª[DEV](https://) \n",
        "\n",
        "### You can find video related in:\n",
        "üì∫[YouTube](https://) \n",
        "\n",
        "### You can find repo related in:\n",
        "üê±‚Äçüèç[GitHub](https://) \n",
        "\n",
        "### You can connect with me in:\n",
        "üß¨[LinkedIn](https://) \n",
        "\n",
        "--- \n"
      ],
      "metadata": {
        "id": "FTsEc81XPR5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resume üßæ\n",
        "\n",
        "I will install Hadoop program and will use a library of Python to write a job that answer the question, how many row exists by each rating?"
      ],
      "metadata": {
        "id": "CGMj2ecitOH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 1st - Install Hadoop üêò\n",
        "\n",
        "I use following command but you can change to get current last version:\n",
        "\n",
        "`!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz`"
      ],
      "metadata": {
        "id": "NHaLUOyzXFMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeK7XRzd_ifg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03f00ea-271b-47f2-d9a0-7392de73baf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-28 20:59:20--  https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695457782 (663M) [application/x-gzip]\n",
            "Saving to: ‚Äòhadoop-3.3.4.tar.gz‚Äô\n",
            "\n",
            "hadoop-3.3.4.tar.gz 100%[===================>] 663.24M  10.7MB/s    in 64s     \n",
            "\n",
            "2022-12-28 21:00:25 (10.4 MB/s) - ‚Äòhadoop-3.3.4.tar.gz‚Äô saved [695457782/695457782]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would can get other version if you need in: https://downloads.apache.org/hadoop/common/ and later replace it in the before command.\n"
      ],
      "metadata": {
        "id": "XZzqhSEUXTPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2nd - Unzip and copy üîì\n",
        "\n",
        "I use following command:\n",
        "\n",
        "`!tar -xzvf hadoop-3.3.4.tar.gz && cp -r hadoop-3.3.4/ /usr/local/`"
      ],
      "metadata": {
        "id": "Op2wZe6yXkyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf hadoop-3.3.4.tar.gz && cp -r hadoop-3.3.4/ /usr/local/"
      ],
      "metadata": {
        "id": "WEm-Duf3juen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 3rd - Set up Hadoop's Java ‚òï\n",
        "\n",
        "I use following command:\n",
        "```\n",
        "#To find the default Java path and add export in hadoop-env.sh\n",
        "JAVA_HOME = !readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "java_home_text = JAVA_HOME[0]\n",
        "java_home_text_command = f\"$ {JAVA_HOME[0]} \"\n",
        "!echo export JAVA_HOME=$java_home_text >>/usr/local/hadoop-3.3.4/etc/hadoop/hadoop-env.sh\n",
        "```"
      ],
      "metadata": {
        "id": "gsWYtGi-X6UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To find the default Java path and add export in hadoop-env.sh\n",
        "JAVA_HOME = !readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "java_home_text = JAVA_HOME[0]\n",
        "java_home_text_command = f\"$ {JAVA_HOME[0]} \"\n",
        "!echo export JAVA_HOME=$java_home_text >>/usr/local/hadoop-3.3.4/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "vTVvI-oRkCUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 4th - Set Hadoop home variables üè°\n",
        "\n",
        "I use following command:\n",
        "```\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ['HADOOP_HOME']=\"/usr/local/hadoop-3.3.4\"\n",
        "os.environ['JAVA_HOME']=java_home_text\n",
        "```"
      ],
      "metadata": {
        "id": "JvjM0JKAZKRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME']=\"/usr/local/hadoop-3.3.4\"\n",
        "os.environ['JAVA_HOME']=java_home_text"
      ],
      "metadata": {
        "id": "Shh47AdJkG4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 5th - Run Hadoop üèÉ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "I use following command:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop`"
      ],
      "metadata": {
        "id": "mAgWqtwWZOf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBvDH-s_kJGo",
        "outputId": "4d45d70f-c75a-4438-d9b2-706720333d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 6th - Create a folder with HDFS üåéüìÇ\n",
        "\n",
        "I use following command:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir ml-100k`\n"
      ],
      "metadata": {
        "id": "5izNMk5xZeD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir ml-100k"
      ],
      "metadata": {
        "id": "xK4NqL-ukMBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### 7th - Remove folder with HDFS ‚ôª\n",
        "\n",
        "Maybe, later you need remove it. To do that you must apply following command:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -rm -r ml-100k`"
      ],
      "metadata": {
        "id": "qs21OIpOdK9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 8th - Getting a dataset to anlyze with Hadoop üíæ\n",
        "\n",
        "I use a dataset from grouplens. You can get other in:\n",
        "http://files.grouplens.org/datasets/\n",
        "\n",
        "This time I use movieslens and you can download it using:\n",
        "\n",
        "`!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip`"
      ],
      "metadata": {
        "id": "S-kUs0yGdQGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DZa2EeHkTLY",
        "outputId": "768dbf4c-6c29-4652-d71f-01ccdcfbfe55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-28 21:01:44--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‚Äòml-100k.zip‚Äô\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  3.45MB/s    in 1.4s    \n",
            "\n",
            "2022-12-28 21:01:46 (3.45 MB/s) - ‚Äòml-100k.zip‚Äô saved [4924029/4924029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To use data extract files. I extract files in path later of -d in command:\n",
        "\n",
        "`!unzip \"/content/ml-100k.zip\" -d \"/content/ml-100k_folder\"`\n"
      ],
      "metadata": {
        "id": "2cHCitoVdjDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/ml-100k.zip\" -d \"/content/ml-100k_folder\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTTLRTBO5c85",
        "outputId": "05780078-e5cb-46c5-8c00-7e896c69eeef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-100k.zip\n",
            "   creating: /content/ml-100k_folder/ml-100k/\n",
            "  inflating: /content/ml-100k_folder/ml-100k/allbut.pl  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/mku.sh  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/README  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.data  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.genre  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.info  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.item  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.occupation  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u.user  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u1.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u1.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u2.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u2.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u3.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u3.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u4.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u4.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u5.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/u5.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ua.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ua.test  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ub.base  \n",
            "  inflating: /content/ml-100k_folder/ml-100k/ub.test  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can move it in actual directory using HDFS like:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -copyFromLocal /content/ml-100k_folder/ml-100k/* ml-100k/`"
      ],
      "metadata": {
        "id": "MNQzQ4ImiiV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -copyFromLocal /content/ml-100k_folder/ml-100k/* ml-100k/"
      ],
      "metadata": {
        "id": "q_x_6l1o5ijg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For list them:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls ml-100k`"
      ],
      "metadata": {
        "id": "RebggKaPjlpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls ml-100k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1azCD7x58MQ",
        "outputId": "d06a78ad-11a0-46d4-c48f-630f465d8796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 23 items\n",
            "-rw-r--r--   1 root root       6750 2022-12-28 21:01 ml-100k/README\n",
            "-rw-r--r--   1 root root        716 2022-12-28 21:01 ml-100k/allbut.pl\n",
            "-rw-r--r--   1 root root        643 2022-12-28 21:01 ml-100k/mku.sh\n",
            "-rw-r--r--   1 root root    1979173 2022-12-28 21:01 ml-100k/u.data\n",
            "-rw-r--r--   1 root root        202 2022-12-28 21:01 ml-100k/u.genre\n",
            "-rw-r--r--   1 root root         36 2022-12-28 21:01 ml-100k/u.info\n",
            "-rw-r--r--   1 root root     236344 2022-12-28 21:01 ml-100k/u.item\n",
            "-rw-r--r--   1 root root        193 2022-12-28 21:01 ml-100k/u.occupation\n",
            "-rw-r--r--   1 root root      22628 2022-12-28 21:01 ml-100k/u.user\n",
            "-rw-r--r--   1 root root    1586544 2022-12-28 21:01 ml-100k/u1.base\n",
            "-rw-r--r--   1 root root     392629 2022-12-28 21:01 ml-100k/u1.test\n",
            "-rw-r--r--   1 root root    1583948 2022-12-28 21:01 ml-100k/u2.base\n",
            "-rw-r--r--   1 root root     395225 2022-12-28 21:01 ml-100k/u2.test\n",
            "-rw-r--r--   1 root root    1582546 2022-12-28 21:01 ml-100k/u3.base\n",
            "-rw-r--r--   1 root root     396627 2022-12-28 21:01 ml-100k/u3.test\n",
            "-rw-r--r--   1 root root    1581878 2022-12-28 21:01 ml-100k/u4.base\n",
            "-rw-r--r--   1 root root     397295 2022-12-28 21:01 ml-100k/u4.test\n",
            "-rw-r--r--   1 root root    1581776 2022-12-28 21:01 ml-100k/u5.base\n",
            "-rw-r--r--   1 root root     397397 2022-12-28 21:01 ml-100k/u5.test\n",
            "-rw-r--r--   1 root root    1792501 2022-12-28 21:01 ml-100k/ua.base\n",
            "-rw-r--r--   1 root root     186672 2022-12-28 21:01 ml-100k/ua.test\n",
            "-rw-r--r--   1 root root    1792476 2022-12-28 21:01 ml-100k/ub.base\n",
            "-rw-r--r--   1 root root     186697 2022-12-28 21:01 ml-100k/ub.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 9th - Installing dependencies to use Python üìö\n",
        "\n",
        "We can install dependency to use MapReduce using:\n",
        "\n",
        "`!pip install mrjob`\n",
        "\n",
        "I recomend learn more about mrjob in: https://mrjob.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "jABHZjDfb7cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InLmyHPg6Igw",
        "outputId": "958c8183-3690-4942-f41a-5829c56610fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 439 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from mrjob) (6.0)\n",
            "Installing collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 10th - Mananging temp folder üíø\n",
        "\n",
        "You can create in anywhere you want."
      ],
      "metadata": {
        "id": "705RlQxfdZ2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a temp folder to use when we run job:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir file:///tmp`"
      ],
      "metadata": {
        "id": "6c7e9zgKcbwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir file:///tmp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GPYExIA6Lod",
        "outputId": "d9804085-94a7-445f-d6e8-34a72095cfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `file:///tmp': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assign permissions to temp folder with:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -mkdir file:///tmp`"
      ],
      "metadata": {
        "id": "johezMFac5Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -chmod 777 file:///tmp"
      ],
      "metadata": {
        "id": "VeHNlGJSMPA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We list files of temp folder:\n",
        "\n",
        "`!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls file:///tmp`"
      ],
      "metadata": {
        "id": "azXyKyVYdL4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop fs -ls file:///tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH8qR8YVMQx4",
        "outputId": "702b5e9a-9985-4905-d157-61a6b2ded919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25 items\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:17 file:///tmp/.ipynb_checkpoints\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:17 file:///tmp/RatingBreakdown.root.20221221.231558.985253\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:23 file:///tmp/RatingBreakdown.root.20221221.232340.510756\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:26 file:///tmp/RatingBreakdown.root.20221221.232626.564574\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:41 file:///tmp/RatingBreakdown.root.20221221.234137.984465\n",
            "-rw-r--r--   1 root root       1184 2022-12-21 22:49 file:///tmp/dap_multiplexer.INFO\n",
            "-rw-r--r--   1 root root       1184 2022-12-21 22:49 file:///tmp/dap_multiplexer.fddb61ece12d.root.log.INFO.20221221-224928.84\n",
            "-rwxr-xr-x   1 root root          0 2022-12-21 22:49 file:///tmp/debugger_2gxkuubmvw\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:01 file:///tmp/hadoop\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 23:01 file:///tmp/hadoop-root\n",
            "drwxr-xr-x   - root root       4096 2022-12-22 00:17 file:///tmp/hsperfdata_root\n",
            "drwx------   - root root       4096 2022-12-21 22:49 file:///tmp/initgoogle_syslog_dir.0\n",
            "-rw-r--r--   1 root root      96602 2022-12-22 00:17 file:///tmp/kernel_manager_proxy.INFO\n",
            "-rw-r--r--   1 root root        483 2022-12-21 22:49 file:///tmp/kernel_manager_proxy.WARNING\n",
            "-rw-r--r--   1 root root      96602 2022-12-22 00:17 file:///tmp/kernel_manager_proxy.fddb61ece12d.root.log.INFO.20221221-224924.36\n",
            "-rw-r--r--   1 root root        483 2022-12-21 22:49 file:///tmp/kernel_manager_proxy.fddb61ece12d.root.log.WARNING.20221221-224929.36\n",
            "drwx------   - root root       4096 2022-12-21 22:58 file:///tmp/pyright-2700-IyoC1mEDekO4\n",
            "drwx------   - root root       4096 2022-12-21 22:58 file:///tmp/pyright-2700-sBT3DYlDp2pl\n",
            "drwx------   - root root       4096 2022-12-21 22:50 file:///tmp/pyright-280-P3GIzLTZc0pC\n",
            "drwx------   - root root       4096 2022-12-21 22:50 file:///tmp/pyright-280-tPEmxYIN25HW\n",
            "drwxr-xr-x   - root root       4096 2022-12-21 22:58 file:///tmp/python-languageserver-cancellation\n",
            "-rw-r--r--   1 root root          0 2022-12-21 23:01 file:///tmp/wrapper.lock.RatingBreakdown.root.20221221.230055.062657\n",
            "-rw-r--r--   1 root root          0 2022-12-21 23:24 file:///tmp/wrapper.lock.RatingBreakdown.root.20221221.232411.513416\n",
            "-rw-r--r--   1 root root          0 2022-12-21 23:34 file:///tmp/wrapper.lock.RatingBreakdown.root.20221221.233339.288251\n",
            "-rw-r--r--   1 root root          0 2022-12-21 23:44 file:///tmp/wrapper.lock.RatingBreakdown.root.20221221.234411.161177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 11th - Creating process to use with MRJOB using Python üêç\n",
        "\n",
        "To create job in Python, you must see structure of dataset to configure jobs.\n",
        "In this case dataset is like:\n",
        "\n",
        "`!head /content/ml-100k/u.data -n -10`"
      ],
      "metadata": {
        "id": "mJylNhSWe4lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head /content/ml-100k/u.data -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCiSlenci44k",
        "outputId": "3ab9838a-6169-4dc6-989c-53121e9a58c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196\t242\t3\t881250949\n",
            "186\t302\t3\t891717742\n",
            "22\t377\t1\t878887116\n",
            "244\t51\t2\t880606923\n",
            "166\t346\t1\t886397596\n",
            "298\t474\t4\t884182806\n",
            "115\t265\t2\t881171488\n",
            "253\t465\t5\t891628467\n",
            "305\t451\t3\t886324817\n",
            "6\t86\t3\t883603013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can get following information of dataset:\n",
        "\n",
        "- First column reference to userID.\n",
        "- Second column reference to movieID.\n",
        "- Third column reference to rating.\n",
        "- Fourth column reference to timestamp."
      ],
      "metadata": {
        "id": "e-4NrtKzjkXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RatingBreakdown.py\n",
        "# import modules\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "\n",
        "# create class inhereted from MRJob\n",
        "class RatingBreakdown(MRJob):\n",
        "  # assign steps, first mapper last reducer\n",
        "  def steps(self):\n",
        "    return [\n",
        "            MRStep(mapper=self.mapper_get_rating,\n",
        "                   reducer=self.reducer_count_ratings)\n",
        "    ]\n",
        "  \n",
        "  # creating mapper, assigning attributes from dataset\n",
        "  def mapper_get_rating(self, _, line):\n",
        "    (userID, movieID, rating, timestamp) = line.split('\\t')\n",
        "    # assign like the key rating and assign each row value 1\n",
        "    yield rating, 1\n",
        "  \n",
        "  # creating reducer, sum \n",
        "  def reducer_count_ratings(self, key, values):\n",
        "    # in function of each key we sum values\n",
        "    yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  RatingBreakdown.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzIoXfzo6UnT",
        "outputId": "6ceffb61-3ca5-417a-ecb0-5bb30848bbef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting RatingBreakdown.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 12th - Running the process üôà\n",
        "\n",
        "Here we run the process specifing some parameters:\n",
        "- Python file program `!python RatingBreakdown.py`\n",
        "- Where is .jar to run hadoop `/usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar`\n",
        "- Temp file and dataset `file:///tmp /content/ml-100k/u.data`\n",
        "\n",
        "When run process, maybe take a few minutes...\n",
        "I run with:\n",
        "\n",
        "`!python RatingBreakdown.py -r hadoop --hadoop-streaming-jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar --hadoop-tmp-dir file:///tmp /content/ml-100k/u.data`"
      ],
      "metadata": {
        "id": "GoVLh7mxlvsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python RatingBreakdown.py -r hadoop --hadoop-streaming-jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar --hadoop-tmp-dir file:///tmp /content/ml-100k/u.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz-cnvfA6u2v",
        "outputId": "943f0ab5-77da-431d-9571-ab5e5a8ca808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for hadoop runner\n",
            "Looking for hadoop binary in /usr/local/hadoop-3.3.4/bin...\n",
            "Found hadoop binary: /usr/local/hadoop-3.3.4/bin/hadoop\n",
            "Using Hadoop version 3.3.4\n",
            "Creating temp directory /tmp/RatingBreakdown.root.20221228.211450.933874\n",
            "uploading working dir files to file:///tmp/RatingBreakdown.root.20221228.211450.933874/files/wd...\n",
            "Copying other local files to file:///tmp/RatingBreakdown.root.20221228.211450.933874/files/\n",
            "Running step 1 of 1...\n",
            "  Loaded properties from hadoop-metrics2.properties\n",
            "  Scheduled Metric snapshot period at 10 second(s).\n",
            "  JobTracker metrics system started\n",
            "  JobTracker metrics system already initialized!\n",
            "  Total input files to process : 1\n",
            "  number of splits:1\n",
            "  Submitting tokens for job: job_local158438747_0001\n",
            "  Executing with tokens: []\n",
            "  Localized file:/tmp/RatingBreakdown.root.20221228.211450.933874/files/wd/RatingBreakdown.py as file:/tmp/hadoop-root/mapred/local/job_local158438747_0001_3dc6f143-0cb2-4ee6-94cf-7772e636163b/RatingBreakdown.py\n",
            "  Creating symlink: /tmp/hadoop-root/mapred/local/job_local158438747_0001_e8f2c343-3a05-4dc3-8330-bf2db9eea164/mrjob.zip <- /content/mrjob.zip\n",
            "  Localized file:/tmp/RatingBreakdown.root.20221228.211450.933874/files/wd/mrjob.zip as file:/tmp/hadoop-root/mapred/local/job_local158438747_0001_e8f2c343-3a05-4dc3-8330-bf2db9eea164/mrjob.zip\n",
            "  Creating symlink: /tmp/hadoop-root/mapred/local/job_local158438747_0001_193a6f56-fb4a-4392-84ed-07a76c45d724/setup-wrapper.sh <- /content/setup-wrapper.sh\n",
            "  Localized file:/tmp/RatingBreakdown.root.20221228.211450.933874/files/wd/setup-wrapper.sh as file:/tmp/hadoop-root/mapred/local/job_local158438747_0001_193a6f56-fb4a-4392-84ed-07a76c45d724/setup-wrapper.sh\n",
            "  The url to track the job: http://localhost:8080/\n",
            "  Running job: job_local158438747_0001\n",
            "  OutputCommitter set in config null\n",
            "  OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "  Waiting for map tasks\n",
            "  Starting task: attempt_local158438747_0001_m_000000_0\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "   Using ResourceCalculatorProcessTree : [ ]\n",
            "  Processing split: file:/tmp/RatingBreakdown.root.20221228.211450.933874/files/u.data:0+1979173\n",
            "  numReduceTasks: 1\n",
            "  (EQUATOR) 0 kvi 26214396(104857584)\n",
            "  mapreduce.task.io.sort.mb: 100\n",
            "  soft limit at 83886080\n",
            "  bufstart = 0; bufvoid = 104857600\n",
            "  kvstart = 26214396; length = 6553600\n",
            "  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "  PipeMapRed exec [/bin/sh, -ex, setup-wrapper.sh, python3, RatingBreakdown.py, --step-num=0, --mapper]\n",
            "  mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "  mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "  map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "  map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "  mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "  map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "  mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "  user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "  R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ __mrjob_PWD=/content\n",
            "+ exec\n",
            "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
            "  R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ export PYTHONPATH=/content/mrjob.zip:/env/python\n",
            "+ exec\n",
            "+ cd /content\n",
            "+ python3 RatingBreakdown.py --step-num=0 --mapper\n",
            "  Records R/W=6704/1\n",
            "  R/W/S=10000/2728/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  Job job_local158438747_0001 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "  R/W/S=100000/95480/0 in:100000=100000/1 [rec/s] out:95480=95480/1 [rec/s]\n",
            "  MRErrorThread done\n",
            "  mapRedFinished\n",
            "  \n",
            "  Starting flush of map output\n",
            "  Spilling map output\n",
            "  bufstart = 0; bufend = 600000; bufvoid = 104857600\n",
            "  kvstart = 26214396(104857584); kvend = 25814400(103257600); length = 399997/6553600\n",
            "  Finished spill 0\n",
            "  Task:attempt_local158438747_0001_m_000000_0 is done. And is in the process of committing\n",
            "  Records R/W=6704/1\n",
            "  Task 'attempt_local158438747_0001_m_000000_0' done.\n",
            "  Final Counters for attempt_local158438747_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2571040\n",
            "\t\tFILE: Number of bytes written=2018411\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=100000\n",
            "\t\tMap output records=100000\n",
            "\t\tMap output bytes=600000\n",
            "\t\tMap output materialized bytes=800006\n",
            "\t\tInput split bytes=118\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=100000\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=335544320\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1994649\n",
            "  Finishing task: attempt_local158438747_0001_m_000000_0\n",
            "  map task executor complete.\n",
            "  Waiting for reduce tasks\n",
            "  Starting task: attempt_local158438747_0001_r_000000_0\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "   Using ResourceCalculatorProcessTree : [ ]\n",
            "  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@32c48be\n",
            "  JobTracker metrics system already initialized!\n",
            "  MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "  attempt_local158438747_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "  localfetcher#1 about to shuffle output of map attempt_local158438747_0001_m_000000_0 decomp: 800002 len: 800006 to MEMORY\n",
            "  Read 800002 bytes from map-output for attempt_local158438747_0001_m_000000_0\n",
            "  closeInMemoryFile -> map-output of size: 800002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->800002\n",
            "  EventFetcher is interrupted.. Returning\n",
            "  1 / 1 copied.\n",
            "  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "  Merging 1 sorted segments\n",
            "  Down to the last merge-pass, with 1 segments left of total size: 799996 bytes\n",
            "  Merged 1 segments, 800002 bytes to disk to satisfy reduce memory limit\n",
            "  Merging 1 files, 800006 bytes from disk\n",
            "  Merging 0 segments, 0 bytes from memory into reduce\n",
            "  Merging 1 sorted segments\n",
            "  Down to the last merge-pass, with 1 segments left of total size: 799996 bytes\n",
            "  1 / 1 copied.\n",
            "  PipeMapRed exec [/bin/sh, -ex, setup-wrapper.sh, python3, RatingBreakdown.py, --step-num=0, --reducer]\n",
            "  mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "  mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "  R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ __mrjob_PWD=/content\n",
            "+ exec\n",
            "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
            "  R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ export PYTHONPATH=/content/mrjob.zip:/env/python\n",
            "+ exec\n",
            "+ cd /content\n",
            "+ python3 RatingBreakdown.py --step-num=0 --reducer\n",
            "   map 100% reduce 0%\n",
            "  R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  Records R/W=100000/1\n",
            "  MRErrorThread done\n",
            "  mapRedFinished\n",
            "  Task:attempt_local158438747_0001_r_000000_0 is done. And is in the process of committing\n",
            "  1 / 1 copied.\n",
            "  Task attempt_local158438747_0001_r_000000_0 is allowed to commit now\n",
            "  Saved output of task 'attempt_local158438747_0001_r_000000_0' to file:/tmp/RatingBreakdown.root.20221228.211450.933874/output\n",
            "  Records R/W=100000/1 > reduce\n",
            "  Task 'attempt_local158438747_0001_r_000000_0' done.\n",
            "  Final Counters for attempt_local158438747_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=4171084\n",
            "\t\tFILE: Number of bytes written=2818478\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=800006\n",
            "\t\tReduce input records=100000\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=100000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=335544320\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=61\n",
            "  Finishing task: attempt_local158438747_0001_r_000000_0\n",
            "  reduce task executor complete.\n",
            "   map 100% reduce 100%\n",
            "  Job job_local158438747_0001 completed successfully\n",
            "  Output directory: file:///tmp/RatingBreakdown.root.20221228.211450.933874/output\n",
            "Counters: 30\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1994649\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=61\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6742124\n",
            "\t\tFILE: Number of bytes written=4836889\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tInput split bytes=118\n",
            "\t\tMap input records=100000\n",
            "\t\tMap output bytes=600000\n",
            "\t\tMap output materialized bytes=800006\n",
            "\t\tMap output records=100000\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce input records=100000\n",
            "\t\tReduce output records=5\n",
            "\t\tReduce shuffle bytes=800006\n",
            "\t\tShuffled Maps =1\n",
            "\t\tSpilled Records=200000\n",
            "\t\tTotal committed heap usage (bytes)=671088640\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "job output is in file:///tmp/RatingBreakdown.root.20221228.211450.933874/output\n",
            "Streaming final output from file:///tmp/RatingBreakdown.root.20221228.211450.933874/output...\n",
            "\"1\"\t6110\n",
            "\"2\"\t11370\n",
            "\"3\"\t27145\n",
            "\"4\"\t34174\n",
            "\"5\"\t21201\n",
            "Removing HDFS temp directory file:///tmp/RatingBreakdown.root.20221228.211450.933874...\n",
            "Removing temp directory /tmp/RatingBreakdown.root.20221228.211450.933874...\n",
            "[Errno 2] No such file or directory: '/tmp/RatingBreakdown.root.20221228.211450.933874'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/runner.py\", line 598, in _cleanup_local_tmp\n",
            "    rmtree(self._local_tmp_dir)\n",
            "  File \"/usr/lib/python3.8/shutil.py\", line 709, in rmtree\n",
            "    onerror(os.lstat, path, sys.exc_info())\n",
            "  File \"/usr/lib/python3.8/shutil.py\", line 707, in rmtree\n",
            "    orig_st = os.lstat(path)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/RatingBreakdown.root.20221228.211450.933874'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 13th - Listing results ü•Ç\n",
        "\n",
        "I run again process and put results in results.txt\n",
        "\n",
        "`!python RatingBreakdown.py /content/ml-100k_folder/ml-100k/u.data >> results.txt`"
      ],
      "metadata": {
        "id": "VoviTaHRnYLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python RatingBreakdown.py /content/ml-100k_folder/ml-100k/u.data >> results.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Ayubm-Ca4X",
        "outputId": "28f9c9b5-b756-445f-a4d7-1bba5f2806d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/RatingBreakdown.root.20221228.212341.761322\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/RatingBreakdown.root.20221228.212341.761322/output\n",
            "Streaming final output from /tmp/RatingBreakdown.root.20221228.212341.761322/output...\n",
            "Removing temp directory /tmp/RatingBreakdown.root.20221228.212341.761322...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use following command to list file: \n",
        "\n",
        "`!cat results.txt`"
      ],
      "metadata": {
        "id": "NIN2_tC8rnGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat results.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swxVQ9f5Cgw1",
        "outputId": "0e6181ba-f5b0-4bb9-c28b-085deb87f657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"5\"\t21201\n",
            "\"4\"\t34174\n",
            "\"1\"\t6110\n",
            "\"2\"\t11370\n",
            "\"3\"\t27145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 14th - Say thanks, give like and share if this has been of help/interest üòÅüññ\n",
        "---\n",
        "## üîóRelated content \n",
        "### You can find post related in: \n",
        "üë®‚Äçüíª[DEV](https://) \n",
        "\n",
        "### You can find video related in:\n",
        "üì∫[YouTube](https://) \n",
        "\n",
        "### You can find repo related in:\n",
        "üê±‚Äçüèç[GitHub](https://) \n",
        "\n",
        "### You can connect with me in:\n",
        "üß¨[LinkedIn](https://) \n",
        "\n",
        "--- \n"
      ],
      "metadata": {
        "id": "LupX9XxCn2Op"
      }
    }
  ]
}